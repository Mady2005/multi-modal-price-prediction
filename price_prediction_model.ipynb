{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13360639,"sourceType":"datasetVersion","datasetId":8463168}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom pandas.errors import ParserError\n\ndef _download_worker(args):\n    \"\"\"\n    Downloads a single image, handles errors, and skips existing files.\n    This is the helper function for our main downloader.\n    \"\"\"\n    image_url, save_path = args\n\n    # Skip if we already have this image\n    if os.path.exists(save_path):\n        return 'skipped'\n\n    try:\n        # Use a timeout to avoid getting stuck on a bad link\n        response = requests.get(image_url, stream=True, timeout=20)\n        # Raise an error if the link is broken (like a 404 Not Found)\n        response.raise_for_status()\n\n        # Save the image to the specified path\n        with open(save_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        return 'success'\n    except requests.exceptions.RequestException:\n        # If anything goes wrong, just mark it as failed and move on\n        return 'failed'\n\ndef download_images_from_csv(csv_path: str, image_link_column: str, filename_column: str, save_folder: str, chunksize: int = 10000):\n    \"\"\"\n    Reads a CSV file in chunks, handling ParserError, and downloads all images.\n\n    Args:\n        csv_path (str): The path to your CSV file in Colab.\n        image_link_column (str): The name of the column containing the image URLs.\n        filename_column (str): The column to use for naming the saved files (e.g., 'sample_id').\n        save_folder (str): The name of the folder where images will be saved.\n        chunksize (int): The number of rows to read at a time from the CSV.\n    \"\"\"\n    print(f\"Attempting to read image links from '{csv_path}' in chunks...\")\n\n    df_list = []\n    try:\n        # Wrap the iterator creation in try-except\n        csv_iterator = pd.read_csv(csv_path, chunksize=chunksize, low_memory=False)\n        for i, chunk in enumerate(csv_iterator):\n            df_list.append(chunk)\n    except FileNotFoundError:\n        print(f\"--- ERROR: The file '{csv_path}' was not found. Please upload it first. ---\")\n        return\n    except ParserError as e:\n        print(f\"--- WARNING: ParserError encountered while setting up or reading a chunk. Processing may be incomplete. Error: {e} ---\")\n        # If the iterator creation or first chunk read fails, df_list might be empty or incomplete\n        # We will proceed with whatever was successfully read, if anything.\n        pass # Allow processing of successfully read chunks if any\n\n    if not df_list:\n        print(\"--- No valid data chunks were read from the CSV. ---\")\n        return\n\n    df = pd.concat(df_list, ignore_index=True)\n    print(f\"Successfully read {len(df)} rows from the CSV (potentially with skipped chunks).\")\n\n    # Create the folder to save images in, if it doesn't exist\n    os.makedirs(save_folder, exist_ok=True)\n    print(f\"Images will be saved in the '{save_folder}' directory.\")\n\n    # Create a list of all download jobs\n    tasks = []\n    for _, row in df.iterrows():\n        image_url = row[image_link_column]\n        # Use the filename_column to create a unique filename, adding .jpg\n        filename = f\"{row[filename_column]}.jpg\"\n        full_save_path = os.path.join(save_folder, filename)\n        tasks.append((image_url, full_save_path))\n\n    # Use ThreadPoolExecutor to download images in parallel (much faster!)\n    print(f\"Starting download of {len(tasks)} images...\")\n    with ThreadPoolExecutor(max_workers=32) as executor:\n        results = list(tqdm(executor.map(_download_worker, tasks), total=len(tasks)))\n\n    # Print a final summary\n    success_count = results.count('success')\n    skipped_count = results.count('skipped')\n    failed_count = results.count('failed')\n    print(\"\\n--- Download Complete! ---\")\n    print(f\"  Successfully downloaded: {success_count}\")\n    print(f\"  Already existed (skipped): {skipped_count}\")\n    print(f\"  Failed to download:      {failed_count}\")\n    print(\"--------------------------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cloud Storage\nfrom google.cloud import storage\nstorage_client = storage.Client(project='notebooked61a9156c')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Define your file and column names\nmy_csv_file = '/kaggle/input/challenge/test.csv'\nurl_column_name = 'image_link'\nid_column_name = 'sample_id'\noutput_folder_name = 'test_images'\n\n# 2. Run the updated function\ndownload_images_from_csv(\n    csv_path=my_csv_file,\n    image_link_column=url_column_name,\n    filename_column=id_column_name,\n    save_folder=output_folder_name\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Define your file and column names\nmy_csv_file = '/kaggle/input/challenge/train.csv'\nurl_column_name = 'image_link'\nid_column_name = 'sample_id'\noutput_folder_name = 'train_images'\n\n# 2. Run the updated function\ndownload_images_from_csv(\n    csv_path=my_csv_file,\n    image_link_column=url_column_name,\n    filename_column=id_column_name,\n    save_folder=output_folder_name\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\n# --- Make sure your DataFrames are loaded ---\ntrain_df = pd.read_csv('/kaggle/input/challenge/train.csv')\ntest_df = pd.read_csv('/kaggle/input/challenge/test.csv')\n# --- 1. IMPORTANT: EDIT THIS LINE ---\n# Replace 'your-dataset-name-here' with the actual name of your dataset folder.\n# You can find this in the \"Data\" panel on the right side of your notebook.\ninput_path = '/kaggle/input/challenge'\n\n# --- 2. Verify that the path is correct ---\nprint(f\"Looking for files in: {input_path}\")\nprint(\"Files found:\", os.listdir(input_path))\n\n\n# --- 3. Build the full paths to your embedding files ---\npath_train_text = os.path.join(input_path, 'train_text_embeddings.npy')\npath_test_text = os.path.join(input_path, 'test_text_embeddings.npy')\npath_train_image = os.path.join(input_path, 'train_image_embeddings.npy')\npath_test_image = os.path.join(input_path, 'test_image_embeddings.npy')\n\n# --- 4. Load all embeddings using the correct full paths ---\ntry:\n    print(\"\\nLoading all saved embeddings...\")\n    train_text_embeddings = np.load(path_train_text)\n    test_text_embeddings = np.load(path_test_text)\n    train_image_embeddings = np.load(path_train_image)\n    test_image_embeddings = np.load(path_test_image)\n\n    print(\"\\nâœ… All embeddings loaded successfully!\")\n    print(f\"Shape of train_text_embeddings: {train_text_embeddings.shape}\")\n    print(f\"Shape of train_image_embeddings: {train_image_embeddings.shape}\")\n\nexcept FileNotFoundError as e:\n    print(f\"\\n--- ERROR ---\")\n    print(f\"A file was not found. Please double-check your input_path.\")\n    print(f\"Original error: {e}\")\n\n\n\n# --- Re-create the item_quantity feature (if needed) ---\nimport re\ndef extract_ipq(text):\n    match = re.search(r'(?:IPQ|Pack of|Count|pack-of|pack|Pack|count)[\\s:]*(\\d+)', str(text), re.IGNORECASE)\n    return int(match.group(1)) if match else 1\n\ntrain_df['item_quantity'] = train_df['catalog_content'].apply(extract_ipq)\ntest_df['item_quantity'] = test_df['catalog_content'].apply(extract_ipq)\n\n# --- Horizontally stack (combine) all features into one matrix ---\nprint(\"Combining all features into final training and test sets...\")\nX_train = np.hstack([\n    train_df[['item_quantity']].values,\n    train_text_embeddings,\n    train_image_embeddings\n])\n\nX_test = np.hstack([\n    test_df[['item_quantity']].values,\n    test_text_embeddings,\n    test_image_embeddings\n])\n\n# --- Define the target variable with log transformation ---\n# This helps the model optimize for the SMAPE metric.\ny_train = np.log1p(train_df['price'])\n\nprint(f\"\\nâœ… Final training data shape: {X_train.shape}\")\nprint(f\"âœ… Final test data shape: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:35:05.132808Z","iopub.execute_input":"2025-10-13T15:35:05.133364Z","iopub.status.idle":"2025-10-13T15:35:17.891837Z","shell.execute_reply.started":"2025-10-13T15:35:05.133342Z","shell.execute_reply":"2025-10-13T15:35:17.891068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\n\n# --- 1. IMPORTANT: EDIT THIS LINE ---\n# Replace 'your-dataset-name-here' with the actual name of your dataset folder.\n# You can find this in the \"Data\" panel on the right side of your notebook.\ninput_path = '/kaggle/input/challenge'\n\n# --- 2. Verify that the path is correct ---\nprint(f\"Looking for files in: {input_path}\")\nprint(\"Files found:\", os.listdir(input_path))\n\n\n# --- 3. Build the full paths to your embedding files ---\npath_train_text = os.path.join(input_path, 'train_text_embeddings.npy')\npath_test_text = os.path.join(input_path, 'test_text_embeddings.npy')\npath_train_image = os.path.join(input_path, 'train_image_embeddings.npy')\npath_test_image = os.path.join(input_path, 'test_image_embeddings.npy')\n\n\n# --- 4. Load all embeddings using the correct full paths ---\ntry:\n    print(\"\\nLoading all saved embeddings...\")\n    train_text_embeddings = np.load(path_train_text)\n    test_text_embeddings = np.load(path_test_text)\n    train_image_embeddings = np.load(path_train_image)\n    test_image_embeddings = np.load(path_test_image)\n\n    print(\"\\nâœ… All embeddings loaded successfully!\")\n    print(f\"Shape of train_text_embeddings: {train_text_embeddings.shape}\")\n    print(f\"Shape of train_image_embeddings: {train_image_embeddings.shape}\")\n\nexcept FileNotFoundError as e:\n    print(f\"\\n--- ERROR ---\")\n    print(f\"A file was not found. Please double-check your input_path.\")\n    print(f\"Original error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:46.107021Z","iopub.execute_input":"2025-10-13T15:33:46.107599Z","iopub.status.idle":"2025-10-13T15:33:52.686013Z","shell.execute_reply.started":"2025-10-13T15:33:46.107573Z","shell.execute_reply":"2025-10-13T15:33:52.685404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\n# --- K-Fold Cross-Validation Setup ---\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# Arrays to store predictions\noof_predictions = np.zeros(X_train.shape[0]) # Predictions on our own training data\ntest_predictions = np.zeros(X_test.shape[0]) # Predictions for the submission file\n\n# --- Training Loop ---\nfor fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n    print(f\"========== FOLD {fold+1} ==========\")\n    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]\n    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    model = lgb.LGBMRegressor(\n        objective='regression_l1',  # MAE is a good objective for SMAPE\n        n_estimators=2000,\n        learning_rate=0.05,\n        num_leaves=32,\n        n_jobs=-1,\n        seed=42\n    )\n\n    model.fit(x_train_fold, y_train_fold,\n              eval_set=[(x_val_fold, y_val_fold)],\n              eval_metric='rmse',\n              callbacks=[lgb.early_stopping(100, verbose=False)])\n\n    # Store predictions\n    oof_predictions[val_index] = model.predict(x_val_fold)\n    test_predictions += model.predict(X_test) / NFOLDS # Average test predictions over the 5 folds\n\nprint(\"\\nâœ… Model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T07:59:47.033917Z","iopub.execute_input":"2025-10-13T07:59:47.034377Z","iopub.status.idle":"2025-10-13T08:21:51.466685Z","shell.execute_reply.started":"2025-10-13T07:59:47.034354Z","shell.execute_reply":"2025-10-13T08:21:51.466098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Inverse transform the log predictions to get actual prices ---\nfinal_predictions = np.expm1(test_predictions)\n\n# --- Create the submission DataFrame ---\nsubmission_df = pd.DataFrame({\n    'sample_id': test_df['sample_id'],\n    'price': final_predictions\n})\n\n# Ensure all prices are positive, as required\nsubmission_df['price'] = submission_df['price'].clip(lower=0.01)\n\n# --- Save the final file ---\nsubmission_df.to_csv('test_out.csv', index=False)\n\nprint(\"ðŸŽ‰ Submission file 'test_out.csv' created successfully!\")\nprint(\"Here's a preview of your submission file:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T08:21:56.92387Z","iopub.execute_input":"2025-10-13T08:21:56.924555Z","iopub.status.idle":"2025-10-13T08:21:57.079959Z","shell.execute_reply.started":"2025-10-13T08:21:56.924527Z","shell.execute_reply":"2025-10-13T08:21:57.079255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Define the SMAPE function ---\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    # Handle the edge case where both true and pred are zero\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    return np.mean(ratio) * 100\n\n# --- 2. Get the true prices from the original dataframe ---\nactual_prices = train_df['price'].values\n\n# --- 3. IMPORTANT: Convert OOF predictions from log scale back to normal prices ---\n# This is a critical step! The model predicted log(price+1).\noof_prices = np.expm1(oof_predictions)\n\n# --- 4. Calculate and print your estimated score ---\nlocal_smape_score = smape(actual_prices, oof_prices)\n\nprint(f\"Your estimated Out-of-Fold SMAPE score is: {local_smape_score:.4f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T08:22:28.829199Z","iopub.execute_input":"2025-10-13T08:22:28.829865Z","iopub.status.idle":"2025-10-13T08:22:28.850986Z","shell.execute_reply.started":"2025-10-13T08:22:28.829844Z","shell.execute_reply":"2025-10-13T08:22:28.850444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List all files in your \"desk\" (the output directory)\n!ls /kaggle/working/","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T07:46:13.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# --- 1. Define the SMAPE function ---\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    # Handle the edge case where both true and pred are zero to avoid division by zero\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    return np.mean(ratio) * 100\n\n# --- 2. Load the training data to get the true prices ---\n# Make sure the path to your train.csv is correct\n# For Kaggle, it would be something like '/kaggle/input/your-dataset-name/train.csv'\ntrain_df = pd.read_csv('/kaggle/working/test_out.csv')\n\nactual_prices = train_df['price'].values\n\n# --- 3. IMPORTANT: Convert OOF predictions back to the price scale ---\n# The model predicted log(price+1), so we need to reverse that.\noof_prices = np.expm1(oof_predictions)\n\n# --- 4. Calculate and print your estimated score ---\nlocal_smape_score = smape(actual_prices, oof_prices)\n\nprint(\"=\"*50)\nprint(f\"Your estimated Out-of-Fold SMAPE score is: {local_smape_score:.4f}%\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T08:23:58.684379Z","iopub.execute_input":"2025-10-13T08:23:58.684888Z","iopub.status.idle":"2025-10-13T08:23:58.728007Z","shell.execute_reply.started":"2025-10-13T08:23:58.684862Z","shell.execute_reply":"2025-10-13T08:23:58.727449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# --- Load your original CSV files ---\n# Make sure the path is correct for your environment (e.g., Kaggle, Colab)\ntry:\n    train_df = pd.read_csv('/kaggle/input/challenge/train.csv')\n    test_df = pd.read_csv('/kaggle/input/challenge/test.csv')\n    print(\"âœ… CSV files loaded successfully.\")\nexcept FileNotFoundError:\n    print(\"--- ERROR: train.csv or test.csv not found. Please check the path. ---\")\n    # You might need to add the full path, e.g., '/kaggle/input/your-dataset/train.csv'\n\n# --- Feature 1: Extract Brand Name (High Impact) ---\ndef find_brand(text):\n    text = str(text).lower()\n    # Add more brands as you discover them in your data\n    brands = ['apple', 'samsung', 'sony', 'nike', 'dell', 'hp', 'lego', 'adidas']\n    for brand in brands:\n        if brand in text:\n            return brand\n    return 'unknown'\n\nprint(\"Extracting brand names...\")\ntrain_df['brand'] = train_df['catalog_content'].apply(find_brand)\ntest_df['brand'] = test_df['catalog_content'].apply(find_brand)\n\n# --- Feature 2: Extract Numerical Specs ---\nprint(\"Extracting numerical specifications...\")\n# Extracts numbers followed by 'GB' (for memory/storage)\ntrain_df['spec_gb'] = train_df['catalog_content'].str.extract(r'(\\d+)\\s*GB', flags=re.IGNORECASE).astype(float).fillna(0)\ntest_df['spec_gb'] = test_df['catalog_content'].str.extract(r'(\\d+)\\s*GB', flags=re.IGNORECASE).astype(float).fillna(0)\n\n# Extracts numbers followed by 'ml' (for liquids)\ntrain_df['spec_ml'] = train_df['catalog_content'].str.extract(r'(\\d+)\\s*ml', flags=re.IGNORECASE).astype(float).fillna(0)\ntest_df['spec_ml'] = test_df['catalog_content'].str.extract(r'(\\d+)\\s*ml', flags=re.IGNORECASE).astype(float).fillna(0)\n\n# --- Convert the 'brand' column from text to numbers ---\nprint(\"Encoding categorical 'brand' feature...\")\ntrain_brands_encoded = pd.get_dummies(train_df['brand'], prefix='brand')\ntest_brands_encoded = pd.get_dummies(test_df['brand'], prefix='brand')\n\n# Align columns to ensure both have the same brand features\ntrain_brands_encoded, test_brands_encoded = train_brands_encoded.align(test_brands_encoded, join='left', axis=1, fill_value=0)\n\nprint(\"\\nâœ… New features created and encoded successfully!\")\nprint(\"Here's a sample of the new features:\")\nprint(train_df[['brand', 'spec_gb', 'spec_ml']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:01.627111Z","iopub.execute_input":"2025-10-13T15:33:01.627782Z","iopub.status.idle":"2025-10-13T15:33:11.145897Z","shell.execute_reply.started":"2025-10-13T15:33:01.627759Z","shell.execute_reply":"2025-10-13T15:33:11.145199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This assumes your original X_train and X_test matrices are already in memory.\n# If not, you'll need to load them from your .npy files first.\n\nprint(\"Combining original features with new features...\")\nX_train_new = np.hstack([\n    X_train,  # Your original 897 features\n    train_brands_encoded.values,\n    train_df[['spec_gb', 'spec_ml']].values\n])\n\nX_test_new = np.hstack([\n    X_test,   # Your original 897 features\n    test_brands_encoded.values,\n    test_df[['spec_gb', 'spec_ml']].values\n])\n\nprint(f\"\\nOld number of features: {X_train.shape[1]}\")\nprint(f\"New number of features: {X_train_new.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:34:15.801965Z","iopub.execute_input":"2025-10-13T15:34:15.802237Z","iopub.status.idle":"2025-10-13T15:34:15.811955Z","shell.execute_reply.started":"2025-10-13T15:34:15.802217Z","shell.execute_reply":"2025-10-13T15:34:15.810823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\n\n# --- K-Fold Cross-Validation Setup ---\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# --- Use NEW variable names to store predictions from our new model ---\noof_predictions_new = np.zeros(X_train_new.shape[0])\ntest_predictions_new = np.zeros(X_test_new.shape[0])\n\n# --- Training Loop ---\n# This will train 5 separate models on your new, feature-rich dataset\nfor fold, (train_index, val_index) in enumerate(kf.split(X_train_new, y_train)):\n    print(f\"========== FOLD {fold+1} ==========\")\n    # Use the NEW data matrices for training and validation\n    x_train_fold, x_val_fold = X_train_new[train_index], X_train_new[val_index]\n    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    # Initialize the LightGBM model (same settings as before for a fair comparison)\n    model = lgb.LGBMRegressor(\n        objective='regression_l1',\n        n_estimators=2000,\n        learning_rate=0.05,\n        num_leaves=32,\n        n_jobs=-1,\n        seed=42,\n        boosting_type='gbdt',\n    )\n\n    # Train the model\n    model.fit(x_train_fold, y_train_fold,\n              eval_set=[(x_val_fold, y_val_fold)],\n              eval_metric='rmse',\n              callbacks=[lgb.early_stopping(100, verbose=False)])\n\n    # Store the new predictions\n    oof_predictions_new[val_index] = model.predict(x_val_fold)\n    test_predictions_new += model.predict(X_test_new) / NFOLDS\n\nprint(\"\\nâœ… New model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T08:35:26.919262Z","iopub.execute_input":"2025-10-13T08:35:26.919966Z","iopub.status.idle":"2025-10-13T08:58:12.618433Z","shell.execute_reply.started":"2025-10-13T08:35:26.919942Z","shell.execute_reply":"2025-10-13T08:58:12.617843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Define the SMAPE function ---\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    return np.mean(ratio) * 100\n\n# --- Get the true prices ---\nactual_prices = train_df['price'].values\n\n# --- Convert NEW OOF predictions back to the price scale ---\noof_prices_new = np.expm1(oof_predictions_new)\n\n# --- Calculate and print your new estimated score ---\nlocal_smape_score_new = smape(actual_prices, oof_prices_new)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Your NEW estimated SMAPE score is: {local_smape_score_new:.4f}%\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T09:58:26.919436Z","iopub.execute_input":"2025-10-13T09:58:26.920014Z","iopub.status.idle":"2025-10-13T09:58:26.930342Z","shell.execute_reply.started":"2025-10-13T09:58:26.91999Z","shell.execute_reply":"2025-10-13T09:58:26.929412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This uses the new OOF predictions you just generated\nerror_df = pd.DataFrame({\n    'actual_price': np.expm1(y_train),\n    'predicted_price': np.expm1(oof_predictions_new),\n    'catalog_content': train_df['catalog_content']\n})\nerror_df['error'] = np.abs(error_df['actual_price'] - error_df['predicted_price'])\n\n# Find your top 10 worst predictions\nworst_errors = error_df.sort_values(by='error', ascending=False)\n\nprint(\"Top 10 Worst Predictions:\")\nprint(worst_errors.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T09:58:11.379999Z","iopub.execute_input":"2025-10-13T09:58:11.380673Z","iopub.status.idle":"2025-10-13T09:58:11.39146Z","shell.execute_reply.started":"2025-10-13T09:58:11.380649Z","shell.execute_reply":"2025-10-13T09:58:11.390388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# This assumes train_df and test_df are already loaded with your previous features\n\nprint(\"Creating new advanced features...\")\n\n# --- Feature 1: A flag for bulk keywords ---\nbulk_keywords = ['kit', 'pallet', 'case', 'bucket', 'pack', 'bulk', 'servings', 'supply', 'bottles']\ndef check_for_bulk(text):\n    text = str(text).lower()\n    for keyword in bulk_keywords:\n        if keyword in text:\n            return 1\n    return 0\n\ntrain_df['is_bulk'] = train_df['catalog_content'].apply(check_for_bulk)\ntest_df['is_bulk'] = test_df['catalog_content'].apply(check_for_bulk)\n\n# --- Feature 2: A better way to get quantity (servings) ---\ndef get_servings(text):\n    text = str(text).lower()\n    match = re.search(r'(\\d+)\\s*servings', text)\n    if match:\n        return int(match.group(1))\n    return 0\n\ntrain_df['total_servings'] = train_df['catalog_content'].apply(get_servings)\ntest_df['total_servings'] = test_df['catalog_content'].apply(get_servings)\n\n# --- Feature 3: Standardized Weight ---\ndef get_weight_in_grams(text):\n    text = str(text).lower()\n    # Pounds to grams (1 lb = 453.592g)\n    match_lb = re.search(r'(\\d+\\.?\\d*)\\s*(lb|pound)', text)\n    if match_lb:\n        return float(match_lb.group(1)) * 453.592\n    \n    # Ounces to grams (1 oz = 28.35g)\n    match_oz = re.search(r'(\\d+\\.?\\d*)\\s*oz', text)\n    if match_oz:\n        return float(match_oz.group(1)) * 28.35\n    \n    return 0\n\ntrain_df['weight_grams'] = train_df['catalog_content'].apply(get_weight_in_grams)\ntest_df['weight_grams'] = test_df['catalog_content'].apply(get_weight_in_grams)\n\nprint(\"âœ… New forensic features created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T09:58:38.860277Z","iopub.execute_input":"2025-10-13T09:58:38.86082Z","iopub.status.idle":"2025-10-13T09:58:48.270019Z","shell.execute_reply.started":"2025-10-13T09:58:38.860793Z","shell.execute_reply":"2025-10-13T09:58:48.26939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This assumes X_train_new and X_test_new from your 56.1 model are in memory\nnew_forensic_features_train = train_df[['is_bulk', 'total_servings', 'weight_grams']].values\nnew_forensic_features_test = test_df[['is_bulk', 'total_servings', 'weight_grams']].values\n\n# Combine everything\nX_train_final = np.hstack([X_train_new, new_forensic_features_train])\nX_test_final = np.hstack([X_test_new, new_forensic_features_test])\n\nprint(f\"Final number of features: {X_train_final.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T09:58:57.259265Z","iopub.execute_input":"2025-10-13T09:58:57.259527Z","iopub.status.idle":"2025-10-13T09:58:58.488816Z","shell.execute_reply.started":"2025-10-13T09:58:57.259507Z","shell.execute_reply":"2025-10-13T09:58:58.48824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\n\n# --- K-Fold Cross-Validation Setup ---\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# --- Use FINAL variable names to store the last set of predictions ---\noof_predictions_final = np.zeros(X_train_final.shape[0])\ntest_predictions_final = np.zeros(X_test_final.shape[0])\n\n# --- Training Loop on the Final Dataset ---\nfor fold, (train_index, val_index) in enumerate(kf.split(X_train_final, y_train)):\n    print(f\"========== FOLD {fold+1} ==========\")\n    # Use the FINAL data matrices for training\n    x_train_fold, x_val_fold = X_train_final[train_index], X_train_final[val_index]\n    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    model = lgb.LGBMRegressor(\n        objective='regression_l1',\n        n_estimators=2000,\n        learning_rate=0.05,\n        num_leaves=32,\n        n_jobs=-1,\n        seed=42,\n        boosting_type='gbdt',\n    )\n\n    model.fit(x_train_fold, y_train_fold,\n              eval_set=[(x_val_fold, y_val_fold)],\n              eval_metric='rmse',\n              callbacks=[lgb.early_stopping(100, verbose=False)])\n\n    # Store the final predictions\n    oof_predictions_final[val_index] = model.predict(x_val_fold)\n    test_predictions_final += model.predict(X_test_final) / NFOLDS\n\nprint(\"\\nâœ… Final model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:27:47.410204Z","iopub.execute_input":"2025-10-13T10:27:47.410896Z","iopub.status.idle":"2025-10-13T10:50:32.319116Z","shell.execute_reply.started":"2025-10-13T10:27:47.410872Z","shell.execute_reply":"2025-10-13T10:50:32.318484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Define the SMAPE function ---\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    return np.mean(ratio) * 100\n\n# --- Get the true prices ---\nactual_prices = train_df['price'].values\n\n# --- Convert FINAL OOF predictions back to the price scale ---\noof_prices_final = np.expm1(oof_predictions_final)\n\n# --- Calculate and print your new estimated score ---\nlocal_smape_score_final = smape(actual_prices, oof_prices_final)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Your FINAL estimated SMAPE score is: {local_smape_score_final:.4f}%\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:04:17.05883Z","iopub.execute_input":"2025-10-13T11:04:17.059408Z","iopub.status.idle":"2025-10-13T11:04:17.067034Z","shell.execute_reply.started":"2025-10-13T11:04:17.059386Z","shell.execute_reply":"2025-10-13T11:04:17.066421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:05:42.938507Z","iopub.execute_input":"2025-10-13T11:05:42.93936Z","iopub.status.idle":"2025-10-13T11:05:46.728565Z","shell.execute_reply.started":"2025-10-13T11:05:42.939333Z","shell.execute_reply":"2025-10-13T11:05:46.727785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = xgb.XGBRegressor(\n    # ... other settings ...\n    tree_method='hist',      # <-- Change this to the standard 'hist'\n    device='cuda',           # <-- Add this line to explicitly use the GPU\n    # ... other settings ...\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:03:30.719225Z","iopub.execute_input":"2025-10-13T10:03:30.719626Z","iopub.status.idle":"2025-10-13T10:03:30.723241Z","shell.execute_reply.started":"2025-10-13T10:03:30.719601Z","shell.execute_reply":"2025-10-13T10:03:30.722519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\n\nmodel = xgb.XGBRegressor(\n    # ... other settings ...\n    tree_method='hist',      # <-- Change this to the standard 'hist'\n    device='cuda',           # <-- Add this line to explicitly use the GPU\n    # ... other settings ...\n)\n\n# --- K-Fold Cross-Validation Setup ---\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# --- Use new variable names for the XGBoost predictions ---\noof_predictions_xgb = np.zeros(X_train_final.shape[0])\ntest_predictions_xgb = np.zeros(X_test_final.shape[0])\n\n# --- Training Loop for XGBoost ---\nfor fold, (train_index, val_index) in enumerate(kf.split(X_train_final, y_train)):\n    print(f\"========== FOLD {fold+1} ==========\")\n    x_train_fold, x_val_fold = X_train_final[train_index], X_train_final[val_index]\n    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    # Initialize the XGBoost model\n    model = xgb.XGBRegressor(\n        objective='reg:squarederror', # Standard objective for regression\n        n_estimators=2000,\n        learning_rate=0.05,\n        tree_method='hist',      # <-- CHANGE THIS\n        device='cuda',           # <-- ADD THIS LINE\n  # VERY IMPORTANT: This tells XGBoost to use the GPU\n        early_stopping_rounds=100, # Equivalent to LightGBM's early stopping\n        n_jobs=-1,\n        seed=42\n    )\n\n    # Train the model\n    model.fit(x_train_fold, y_train_fold,\n              eval_set=[(x_val_fold, y_val_fold)],\n              verbose=False) # Hides the training progress for each iteration\n\n    # Store the XGBoost predictions\n    oof_predictions_xgb[val_index] = model.predict(x_val_fold)\n    test_predictions_xgb += model.predict(X_test_final) / NFOLDS\n\nprint(\"\\nâœ… XGBoost model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:05:56.042957Z","iopub.execute_input":"2025-10-13T11:05:56.043253Z","iopub.status.idle":"2025-10-13T11:12:04.12269Z","shell.execute_reply.started":"2025-10-13T11:05:56.043226Z","shell.execute_reply":"2025-10-13T11:12:04.121827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# --- Step 1: Define the SMAPE function ---\ndef smape(y_true, y_pred):\n    \"\"\"Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    return np.mean(ratio) * 100\n\n# --- Step 2: Load the training data to get the true prices ---\n# Make sure the path to your train.csv is correct for your environment.\ntry:\n    train_df = pd.read_csv('/kaggle/input/challenge/train.csv')\n    actual_prices = train_df['price'].values\n    print(\"âœ… Training data loaded successfully.\")\nexcept FileNotFoundError:\n    print(\"--- ERROR: 'train.csv' not found. Please check the file path. ---\")\n    assert False, \"File not found. Correct the path.\"\n\n# ===================================================================\n# IMPORTANT: This assumes 'oof_predictions_final' (from LightGBM) and \n# 'oof_predictions_xgb' (from XGBoost) exist in your notebook's memory.\n# ===================================================================\n\n# --- Step 3: Recalculate the LightGBM score to define the variable ---\nprint(\"Calculating score for LightGBM model...\")\noof_prices_final = np.expm1(oof_predictions_final) # <-- From your LGBM model\nlocal_smape_score_final = smape(actual_prices, oof_prices_final)\n\n# --- Step 4: Calculate the XGBoost score ---\nprint(\"Calculating score for XGBoost model...\")\noof_prices_xgb = np.expm1(oof_predictions_xgb) # <-- From your XGBoost model\nlocal_smape_score_xgb = smape(actual_prices, oof_prices_xgb)\n\n# --- Step 5: Print the final comparison ---\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Your LightGBM (Final) Score was: {local_smape_score_final:.4f}%\")\nprint(f\"Your NEW XGBoost Score is:        {local_smape_score_xgb:.4f}%\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:19:09.403485Z","iopub.execute_input":"2025-10-13T11:19:09.403735Z","iopub.status.idle":"2025-10-13T11:19:10.4834Z","shell.execute_reply.started":"2025-10-13T11:19:09.403716Z","shell.execute_reply":"2025-10-13T11:19:10.482803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Blend the Out-of-Fold (OOF) predictions to check the ensemble score ---\n# This assumes 'oof_predictions_final' (from LGBM) and 'oof_predictions_xgb' are in memory.\noof_predictions_ensemble = (oof_predictions_final * 0.5) + (oof_predictions_xgb * 0.5)\n\n# Convert the blended log predictions back to the price scale\noof_prices_ensemble = np.expm1(oof_predictions_ensemble)\n\n# Calculate the final score for the ensemble\nlocal_smape_score_ensemble = smape(actual_prices, oof_prices_ensemble)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Your LightGBM Score:       {local_smape_score_final:.4f}%\")\nprint(f\"Your XGBoost Score:        {local_smape_score_xgb:.4f}%\")\nprint(f\"Your FINAL ENSEMBLE Score is: {local_smape_score_ensemble:.4f}%\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:22:55.928435Z","iopub.execute_input":"2025-10-13T11:22:55.928975Z","iopub.status.idle":"2025-10-13T11:22:55.938162Z","shell.execute_reply.started":"2025-10-13T11:22:55.928942Z","shell.execute_reply":"2025-10-13T11:22:55.937235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Blend the final test predictions for the submission file ---\n# This assumes 'test_predictions_final' (from LGBM) and 'test_predictions_xgb' are in memory.\ntest_predictions_ensemble = (test_predictions_final * 0.5) + (test_predictions_xgb * 0.5)\n\n# Inverse transform the blended test predictions\nfinal_predictions_ensemble = np.expm1(test_predictions_ensemble)\n\n# Create the new submission DataFrame\nsubmission_df_ensemble = pd.DataFrame({\n    'sample_id': test_df['sample_id'],\n    'price': final_predictions_ensemble\n})\n\n# Ensure all prices are positive\nsubmission_df_ensemble['price'] = submission_df_ensemble['price'].clip(lower=0.01)\n\n# Save the final file\nsubmission_df_ensemble.to_csv('test_out_ensemble.csv', index=False)\n\nprint(\"\\nðŸŽ‰ Final ensemble submission file 'test_out_ensemble.csv' created successfully!\")\nprint(\"Here's a preview:\")\nprint(submission_df_ensemble.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:24:12.513308Z","iopub.execute_input":"2025-10-13T11:24:12.513779Z","iopub.status.idle":"2025-10-13T11:24:12.673637Z","shell.execute_reply.started":"2025-10-13T11:24:12.513754Z","shell.execute_reply":"2025-10-13T11:24:12.672978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# We will look for the 2000 most important single words, word pairs, and triplets\ntfidf = TfidfVectorizer(\n    ngram_range=(1, 3), # (single words, pairs, triplets)\n    max_features=2000,\n    stop_words='english'\n)\n\nprint(\"Fitting TF-IDF on training data...\")\nX_train_tfidf = tfidf.fit_transform(train_df['catalog_content'].fillna(''))\n\nprint(\"Transforming test data with TF-IDF...\")\nX_test_tfidf = tfidf.transform(test_df['catalog_content'].fillna(''))\n\n# --- Rebuild your FINAL dataset, now with TF-IDF features ---\n# Convert the sparse TF-IDF matrix to a dense array for hstack\nX_train_final_v2 = np.hstack([X_train_final, X_train_tfidf.toarray()])\nX_test_final_v2 = np.hstack([X_test_final, X_test_tfidf.toarray()])\n\nprint(f\"Final feature count is now: {X_train_final_v2.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:26:26.838658Z","iopub.execute_input":"2025-10-13T11:26:26.839294Z","iopub.status.idle":"2025-10-13T11:27:29.935531Z","shell.execute_reply.started":"2025-10-13T11:26:26.839271Z","shell.execute_reply":"2025-10-13T11:27:29.934778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:38:17.413051Z","iopub.execute_input":"2025-10-13T11:38:17.413419Z","iopub.status.idle":"2025-10-13T11:38:20.846084Z","shell.execute_reply.started":"2025-10-13T11:38:17.413396Z","shell.execute_reply":"2025-10-13T11:38:20.845333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# This assumes your final, most feature-rich dataset (X_train_final) is loaded.\n# And your target variable y_train is also loaded.\n# And the 'smape' function is defined.\n\ndef objective(trial):\n    \"\"\"\n    This function defines one experiment for Optuna to run.\n    \"\"\"\n    # 1. Suggest a set of hyperparameters for this trial\n    params = {\n        'objective': 'regression_l1',\n        'metric': 'rmse',\n        'n_estimators': 2000,\n        'verbosity': -1,\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n    }\n\n    # 2. Train a model with these params on a validation set\n    # We use a simple split here to make each trial fast.\n    X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n        X_train_final, y_train, test_size=0.2, random_state=42\n    )\n\n    model = lgb.LGBMRegressor(**params) # The ** unpacks the dictionary of params\n    model.fit(X_train_sub, y_train_sub,\n              eval_set=[(X_val, y_val)],\n              callbacks=[lgb.early_stopping(100, verbose=False)])\n\n    # 3. Make predictions and return the SMAPE score\n    preds = model.predict(X_val)\n    preds = np.expm1(preds) # Convert back from log scale\n    y_val_prices = np.expm1(y_val) # Convert true values back too\n\n    score = smape(y_val_prices, preds)\n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:39:18.333792Z","iopub.execute_input":"2025-10-13T11:39:18.334352Z","iopub.status.idle":"2025-10-13T11:39:18.340995Z","shell.execute_reply.started":"2025-10-13T11:39:18.334329Z","shell.execute_reply":"2025-10-13T11:39:18.340139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a study. We want to 'minimize' the SMAPE score.\nstudy = optuna.create_study(direction='minimize')\n\n# Start the optimization process. It will run the 'objective' function 50 times.\n# This will take some time.\nstudy.optimize(objective, n_trials=50)\n\n# --- After it's done, get the results ---\nbest_params_lgbm = study.best_params\nbest_score = study.best_value\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Best SMAPE score found: {best_score:.4f}%\")\nprint(\"Best parameters found:\")\nprint(best_params_lgbm)\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:39:57.348861Z","iopub.execute_input":"2025-10-13T11:39:57.349158Z","iopub.status.idle":"2025-10-13T14:39:57.182729Z","shell.execute_reply.started":"2025-10-13T11:39:57.349135Z","shell.execute_reply":"2025-10-13T14:39:57.181634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# This assumes your train_df and test_df are loaded\n\nprint(\"Creating powerful TF-IDF features...\")\n\n# We will look for the 2000 most important single words, word pairs, and triplets\ntfidf = TfidfVectorizer(\n    ngram_range=(1, 3), # Use single words, pairs, and triplets\n    max_features=2000,  # Limit to the top 2000 most important phrases\n    stop_words='english'\n)\n\n# Fit on the training data and transform both train and test data\nX_train_tfidf = tfidf.fit_transform(train_df['catalog_content'].fillna(''))\nX_test_tfidf = tfidf.transform(test_df['catalog_content'].fillna(''))\n\nprint(\"âœ… TF-IDF features created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:43:01.139446Z","iopub.execute_input":"2025-10-13T14:43:01.139955Z","iopub.status.idle":"2025-10-13T14:43:53.058682Z","shell.execute_reply.started":"2025-10-13T14:43:01.139928Z","shell.execute_reply":"2025-10-13T14:43:53.058054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the sparse TF-IDF matrix to a dense array so we can combine it\nX_train_tfidf_dense = X_train_tfidf.toarray()\nX_test_tfidf_dense = X_test_tfidf.toarray()\n\n# --- Rebuild your FINAL dataset, now with TF-IDF features ---\nX_train_ultimate = np.hstack([X_train_final, X_train_tfidf_dense])\nX_test_ultimate = np.hstack([X_test_final, X_test_tfidf_dense])\n\nprint(f\"Your ultimate feature count is now: {X_train_ultimate.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:41:25.778981Z","iopub.execute_input":"2025-10-13T14:41:25.779629Z","iopub.status.idle":"2025-10-13T14:41:37.93942Z","shell.execute_reply.started":"2025-10-13T14:41:25.779602Z","shell.execute_reply":"2025-10-13T14:41:37.938771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\n\n# This assumes your ultimate datasets are in memory:\n# X_train_ultimate, X_test_ultimate, and y_train\n\n# ===================================================================\n# Part 1: Re-Train LightGBM on the Ultimate Dataset\n# ===================================================================\nprint(\"--- Training LightGBM on Ultimate Features ---\")\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\noof_predictions_lgbm_ult = np.zeros(X_train_ultimate.shape[0])\ntest_predictions_lgbm_ult = np.zeros(X_test_ultimate.shape[0])\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X_train_ultimate, y_train)):\n    print(f\"LGBM Fold {fold+1}/{NFOLDS}\")\n    x_train_fold, x_val_fold = X_train_ultimate[train_index], X_train_ultimate[val_index]\n    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    model_lgbm = lgb.LGBMRegressor(\n        objective='regression_l1', n_estimators=2000, learning_rate=0.05,\n        num_leaves=32, n_jobs=-1, seed=42\n    )\n    model_lgbm.fit(x_train_fold, y_train_fold,\n                   eval_set=[(x_val_fold, y_val_fold)],\n                   eval_metric='rmse',\n                   callbacks=[lgb.early_stopping(100, verbose=False)])\n\n    oof_predictions_lgbm_ult[val_index] = model_lgbm.predict(x_val_fold)\n    test_predictions_lgbm_ult += model_lgbm.predict(X_test_ultimate) / NFOLDS\n\n# ===================================================================\n# Part 2: Re-Train XGBoost on the Ultimate Dataset\n# ===================================================================\nprint(\"\\n--- Training XGBoost on Ultimate Features ---\")\noof_predictions_xgb_ult = np.zeros(X_train_ultimate.shape[0])\ntest_predictions_xgb_ult = np.zeros(X_test_ultimate.shape[0])\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X_train_ultimate, y_train)):\n    print(f\"XGBoost Fold {fold+1}/{NFOLDS}\")\n    x_train_fold, x_val_fold = X_train_ultimate[train_index], X_train_ultimate[val_index]\n    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    model_xgb = xgb.XGBRegressor(\n        objective='reg:squarederror', n_estimators=2000, learning_rate=0.05,\n        tree_method='hist', device='cuda', early_stopping_rounds=100,\n        n_jobs=-1, seed=42\n    )\n    model_xgb.fit(x_train_fold, y_train_fold,\n                  eval_set=[(x_val_fold, y_val_fold)], verbose=False)\n\n    oof_predictions_xgb_ult[val_index] = model_xgb.predict(x_val_fold)\n    test_predictions_xgb_ult += model_xgb.predict(X_test_ultimate) / NFOLDS\n\n# ===================================================================\n# Part 3: Calculate Scores and Create the Final Ensemble\n# ===================================================================\n\n# --- Define the SMAPE function ---\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    return np.mean(ratio) * 100\n\n# --- Get true prices ---\nactual_prices = np.expm1(y_train)\n\n# --- Calculate individual model scores ---\nscore_lgbm_ult = smape(actual_prices, np.expm1(oof_predictions_lgbm_ult))\nscore_xgb_ult = smape(actual_prices, np.expm1(oof_predictions_xgb_ult))\n\n# --- Create the ensemble prediction by averaging ---\noof_predictions_ensemble_ult = (oof_predictions_lgbm_ult * 0.5) + (oof_predictions_xgb_ult * 0.5)\nscore_ensemble_ult = smape(actual_prices, np.expm1(oof_predictions_ensemble_ult))\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"New LightGBM Score:   {score_lgbm_ult:.4f}%\")\nprint(f\"New XGBoost Score:    {score_xgb_ult:.4f}%\")\nprint(f\"ULTIMATE ENSEMBLE SCORE: {score_ensemble_ult:.4f}%\")\nprint(\"=\"*50)\n\n# ===================================================================\n# Part 4: Create the Final Submission File\n# ===================================================================\n\n# --- Blend the test predictions ---\ntest_predictions_ensemble_ult = (test_predictions_lgbm_ult * 0.5) + (test_predictions_xgb_ult * 0.5)\nfinal_predictions = np.expm1(test_predictions_ensemble_ult)\n\n# --- Create the DataFrame ---\nsubmission_df_ultimate = pd.DataFrame({\n    'sample_id': test_df['sample_id'],\n    'price': final_predictions\n})\nsubmission_df_ultimate['price'] = submission_df_ultimate['price'].clip(lower=0.01)\n\n# --- Save the file ---\nsubmission_df_ultimate.to_csv('test_out_ultimate.csv', index=False)\n\nprint(\"\\nðŸŽ‰ Final submission file 'test_out_ultimate.csv' created successfully!\")\nprint(\"Here's a preview:\")\nprint(submission_df_ultimate.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:45:02.969232Z","iopub.execute_input":"2025-10-13T14:45:02.969739Z","execution_failed":"2025-10-13T15:00:01.345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# Step 1: Setup - All Necessary Imports\n# ===================================================================\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, RandomizedSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import uniform, randint\nfrom tqdm import tqdm\n\nprint(\"âœ… All libraries imported successfully.\")\n\n# ===================================================================\n# Step 2: Configuration & Data Loading\n# ===================================================================\n\n# --- âš ï¸ IMPORTANT: EDIT THIS PATH ---\n# Set the path to the folder containing all your data files (.csv and .npy).\n# For Kaggle: '/kaggle/input/your-dataset-name/'\ndata_path = '/kaggle/input/challenge'\n\ntry:\n    print(\"Loading core CSV and embedding files...\")\n    train_df = pd.read_csv(os.path.join(data_path, '/kaggle/input/challenge/train.csv'))\n    test_df = pd.read_csv(os.path.join(data_path, '/kaggle/input/challenge/test.csv'))\n    y_train = np.log1p(train_df['price'])\n    \n    train_text_embeddings = np.load(os.path.join(data_path, 'train_text_embeddings.npy'))\n    test_text_embeddings = np.load(os.path.join(data_path, 'test_text_embeddings.npy'))\n    train_image_embeddings = np.load(os.path.join(data_path, 'train_image_embeddings.npy'))\n    test_image_embeddings = np.load(os.path.join(data_path, 'test_image_embeddings.npy'))\n    print(\"âœ… All data loaded successfully.\")\nexcept FileNotFoundError as e:\n    print(f\"--- ERROR: A file was not found in '{data_path}'. Please check the path. ---\")\n    print(f\"Missing file: {e.filename}\")\n    assert False, \"Execution stopped.\"\n\n# ===================================================================\n# Step 3: Create All Features (Parsed + TF-IDF)\n# ===================================================================\nprint(\"\\n--- Creating all features ---\")\n# (Functions for feature engineering)\ndef find_brand(text):\n    text = str(text).lower(); brands = ['apple', 'samsung', 'sony', 'nike', 'dell', 'hp'];\n    for brand in brands:\n        if brand in text: return brand\n    return 'unknown'\ndef check_for_bulk(text):\n    text = str(text).lower(); bulk_keywords = ['kit', 'pallet', 'case', 'bucket', 'pack'];\n    for keyword in bulk_keywords:\n        if keyword in text: return 1\n    return 0\ndef extract_ipq(text):\n    match = re.search(r'(?:IPQ|Pack of|Count)[\\s:]*(\\d+)', str(text), re.IGNORECASE)\n    return int(match.group(1)) if match else 1\n\nfor df in [train_df, test_df]:\n    df['brand'] = df['catalog_content'].apply(find_brand)\n    df['is_bulk'] = df['catalog_content'].apply(check_for_bulk)\n    df['item_quantity'] = df['catalog_content'].apply(extract_ipq)\n\ntrain_brands_encoded = pd.get_dummies(train_df['brand'], prefix='brand'); test_brands_encoded = pd.get_dummies(test_df['brand'], prefix='brand')\ntrain_brands_encoded, test_brands_encoded = train_brands_encoded.align(test_brands_encoded, join='left', axis=1, fill_value=0)\nparsed_features_train = train_df[['item_quantity', 'is_bulk']].values\nparsed_features_test = test_df[['item_quantity', 'is_bulk']].values\n\ntfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=2000, stop_words='english')\nX_train_tfidf = tfidf.fit_transform(train_df['catalog_content'].fillna('')).toarray()\nX_test_tfidf = tfidf.transform(test_df['catalog_content'].fillna('')).toarray()\n\n# --- Combine all features into one final matrix ---\nX_train_final = np.hstack([parsed_features_train, train_brands_encoded.values, train_text_embeddings, train_image_embeddings, X_train_tfidf])\nX_test_final = np.hstack([parsed_features_test, test_brands_encoded.values, test_text_embeddings, test_image_embeddings, X_test_tfidf])\nprint(f\"âœ… Final dataset created. Total features: {X_train_final.shape[1]}\")\n\n# ===================================================================\n# Step 4: Hyperparameter Tuning with Random Search âš™ï¸\n# ===================================================================\nprint(\"\\n--- Starting Hyperparameter Tuning with Random Search ---\")\nprint(\"This will test 50 random combinations to find the best settings. This will take time.\")\n\n# 1. Define the parameter \"distributions\" to sample from\nparam_dist = {\n    'learning_rate': uniform(0.01, 0.09), # Randomly pick a float between 0.01 and 0.1\n    'num_leaves': randint(30, 100),       # Randomly pick an integer between 30 and 100\n    'max_depth': randint(8, 25),\n    'subsample': uniform(0.6, 0.4),       # Range is start, (end-start)\n    'colsample_bytree': uniform(0.6, 0.4)\n}\n\n# 2. Set up and run the search\nrandom_search = RandomizedSearchCV(\n    estimator=lgb.LGBMRegressor(objective='regression_l1', n_estimators=2000, n_jobs=-1, seed=42),\n    param_distributions=param_dist,\n    n_iter=50, # Number of random combinations to try\n    scoring='neg_root_mean_squared_error',\n    cv=3, # Use 3-fold CV for speed during tuning\n    verbose=1,\n    random_state=42\n)\nrandom_search.fit(X_train_final, y_train)\n\n# 3. Get the best parameters found\nbest_params = random_search.best_params_\nprint(\"\\nâœ… Random Search complete!\")\nprint(\"Best parameters found:\")\nprint(best_params)\n\n# ===================================================================\n# Step 5: Final Training with Best Parameters ðŸš€\n# ===================================================================\nprint(\"\\n--- Training final model using the best parameters found ---\")\n\n# Add the fixed parameters to the dictionary found by the search\nbest_params['objective'] = 'regression_l1'\nbest_params['n_estimators'] = 2000\nbest_params['n_jobs'] = -1\nbest_params['seed'] = 42\n\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\noof_predictions = np.zeros(len(train_df))\ntest_predictions = np.zeros(len(test_df))\n\nfor fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X_train_final, y_train), total=NFOLDS, desc=\"Final Training Folds\")):\n    X_train, X_val = X_train_final[train_idx], X_train_final[val_idx]\n    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # Initialize the model WITH THE BEST PARAMETERS\n    model = lgb.LGBMRegressor(**best_params)\n    model.fit(X_train, y_train_fold,\n              eval_set=[(X_val, y_val_fold)],\n              eval_metric='rmse',\n              callbacks=[lgb.early_stopping(100, verbose=False)])\n    \n    oof_predictions[val_idx] = model.predict(X_val)\n    test_predictions += model.predict(X_test_final) / NFOLDS\n\n# ===================================================================\n# Step 6: Evaluate and Create Submission ðŸ“„\n# ===================================================================\ndef smape(y_true, y_pred):\n    num = np.abs(y_pred - y_true); den = (np.abs(y_true) + np.abs(y_pred)) / 2\n    ratio = np.where(den == 0, 0, num / den)\n    return np.mean(ratio) * 100\n\nfinal_score = smape(train_df['price'].values, np.expm1(oof_predictions))\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Final Estimated OOF SMAPE Score: {final_score:.4f}%\")\nprint(\"=\"*50)\n\nfinal_predictions = np.expm1(test_predictions)\nsubmission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': final_predictions})\nsubmission_df['price'] = submission_df['price'].clip(lower=0.01)\nsubmission_df.to_csv('submission_tuned.csv', index=False)\n\nprint(\"\\nðŸŽ‰ Final submission file 'submission_tuned.csv' created successfully!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:42:53.042951Z","iopub.execute_input":"2025-10-13T15:42:53.0436Z","iopub.status.idle":"2025-10-13T18:13:50.664009Z","shell.execute_reply.started":"2025-10-13T15:42:53.043575Z","shell.execute_reply":"2025-10-13T18:13:50.662978Z"}},"outputs":[],"execution_count":null}]}